# Pipeline ETL e Qualidade de Dados na AWS (Pandas + Boto3) ğŸ›¡ï¸

## ğŸ¯ O Problema de NegÃ³cio
Em projetos de migraÃ§Ã£o de dados, Ã© comum receber arquivos de **sistemas legados** contendo inconsistÃªncias crÃ­ticas (datas invÃ¡lidas, valores negativos, chaves primÃ¡rias nulas). Ingerir esses dados diretamente em bancos de produÃ§Ã£o ou dashboards gera mÃ©tricas erradas e desconfianÃ§a na Ã¡rea de dados.

## ğŸ’¡ A SoluÃ§Ã£o
Desenvolvi um script de engenharia de dados (ETL) que atua como um "Firewall de Qualidade". Ele lÃª os dados brutos do Data Lake (S3), aplica regras de validaÃ§Ã£o rÃ­gidas e separa os registros automaticamente em camadas, garantindo que apenas dados confiÃ¡veis cheguem Ã  etapa final.

## ğŸ› ï¸ Tecnologias Utilizadas
* **Linguagem:** Python 3.11
* **Engenharia de Dados:** Pandas (ManipulaÃ§Ã£o de DataFrames)
* **Cloud & OrquestraÃ§Ã£o:** AWS SDK (Boto3)
* **Armazenamento:** Amazon S3
* **Arquitetura:** Medallion Architecture Simplificada (Bronze/Raw â†’ Silver/Trusted)

## âš™ï¸ Regras de ValidaÃ§Ã£o Implementadas
O pipeline verifica cada linha do dataset contra as seguintes regras de negÃ³cio:
1.  **Sanidade de IDs:** Registros sem identificador Ãºnico (ID) sÃ£o rejeitados.
2.  **ConsistÃªncia Financeira:** O valor da venda deve ser numÃ©rico e maior que zero (Revenue Integrity).
3.  **ValidaÃ§Ã£o Temporal:** Datas devem seguir formato vÃ¡lido (YYYY-MM-DD); registros com datas corrompidas ou inexistentes sÃ£o descartados.

## ğŸ“¸ Resultados do Processamento

### 1. Arquitetura de Pastas (SeparaÃ§Ã£o AutomÃ¡tica)
O script criou automaticamente a estrutura no S3 para segregar os dados baseado na qualidade:
* `raw/`: Entrada bruta (com erros).
* `trusted/`: Dados higienizados e aprovados.
* `rejected/`: Dados reprovados (Quarentena para auditoria).

![Estrutura de Pastas no S3](s3_camadas.png)

### 2. Dados Limpos (Camada Trusted)
Abaixo, o resultado final do processamento. O script removeu corretamente as linhas com datas invÃ¡lidas e valores negativos, restando apenas os dados consistentes (Notebook, Headset):

![Dados Limpos no VS Code](dados_limpos.png)

## ğŸš€ Como Executar
1. Clone este repositÃ³rio.
2. Configure as credenciais AWS no script `etl_qualidade.py` (ou via variÃ¡veis de ambiente).
3. Garanta que o arquivo `vendas_sujas.csv` esteja na pasta `raw/` do bucket S3 alvo.
4. Instale as dependÃªncias:
   ```bash
   pip install pandas boto3
